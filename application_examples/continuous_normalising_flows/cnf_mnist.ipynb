{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.557136Z",
     "start_time": "2023-12-10T16:35:44.827011Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as tforms\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import modelling.odenvp as odenvp\n",
    "from modelling.layers.cnf import CNF\n",
    "from modelling.layers.odefunc import ODEfunc\n",
    "from application_examples.helpers.training import RunningAverageMeter\n",
    "from modelling.utils import count_total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "args = {\n",
    "    'resume': None,     # Change to directory where the model is stored to resume training\n",
    "    'begin_epoch': 1,   # Change to last epoch to resume training\n",
    "    'dims': \"16, 16, 16\",\n",
    "    'strides': \"1, 1, 1, 1\",\n",
    "    'num_blocks': 1,\n",
    "    'num_epochs': 500,\n",
    "    'val_freq': 5,\n",
    "    'batch_size': 200,\n",
    "    'test_batch_size': 200,\n",
    "    'nonlinearity': 'softplus',\n",
    "    'alpha': 1e-6,\n",
    "    'time_length': 1.0,\n",
    "    'warmup_iters': 1000,\n",
    "    'lr': 1e-3,\n",
    "    'solver': 'dopri5',\n",
    "    'atol': 1e-5,\n",
    "    'rtol': 1e-5,\n",
    "    'step_size': None,\n",
    "    'weight_decay': 0.0,\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.558407Z",
     "start_time": "2023-12-10T16:35:46.556859Z"
    }
   },
   "id": "185e6f8ec6f1ad4a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9c5444417952e864"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def add_noise(x):\n",
    "    noise = x.new().resize_as_(x).uniform_()\n",
    "    x = x * 255 + noise\n",
    "    x = x / 256\n",
    "    return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.559757Z",
     "start_time": "2023-12-10T16:35:46.558787Z"
    }
   },
   "id": "5b3c30869e1d4518"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def get_dataset():\n",
    "    trans = lambda im_size: tforms.Compose([\n",
    "        tforms.Resize(im_size),\n",
    "        tforms.ToTensor(),\n",
    "        add_noise\n",
    "    ])\n",
    "    \n",
    "    im_dim = 1\n",
    "    im_size = 28\n",
    "    train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
    "    test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
    "    \n",
    "    data_shape = (im_dim, im_size, im_size)\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        dataset=test_set,\n",
    "        batch_size=args['test_batch_size'],\n",
    "        shuffle=False\n",
    "    )\n",
    "    \n",
    "    return train_set, test_loader, data_shape\n",
    "train_set, test_loader, data_shape = get_dataset()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.611503Z",
     "start_time": "2023-12-10T16:35:46.562093Z"
    }
   },
   "id": "3965ffc0c4a9415d"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def get_train_loader(train_set):\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set,\n",
    "        batch_size=args['batch_size'],\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    print(\"Using batch size {}. Total {} iterations/epoch.\".format(args['batch_size'], len(train_loader)))\n",
    "    return train_loader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.612697Z",
     "start_time": "2023-12-10T16:35:46.610829Z"
    }
   },
   "id": "17e73a0d7ba69d26"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "adb74548961e703b"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 3 scales\n"
     ]
    }
   ],
   "source": [
    "hidden_dims = tuple(map(int, args['dims'].split(',')))\n",
    "strides = tuple(map(int, args['strides'].split(',')))\n",
    "\n",
    "model = odenvp.ODENVP(\n",
    "    input_size=(args['batch_size'], *data_shape),\n",
    "    n_blocks=args['num_blocks'],\n",
    "    intermediate_dims=hidden_dims,\n",
    "    nonlinearity=args['nonlinearity'],\n",
    "    alpha=args['alpha'],\n",
    "    cnf_kwargs={'T': args['time_length'], 'train_T': True } #, 'regularization_fns': ()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.623418Z",
     "start_time": "2023-12-10T16:35:46.613226Z"
    }
   },
   "id": "8c3e7391b315ede5"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def set_cnf_options(args, model):\n",
    "\n",
    "    def _set(module):\n",
    "        if isinstance(module, CNF):\n",
    "            # Set training settings\n",
    "            module.solver = args['solver']\n",
    "            module.atol = args['atol']\n",
    "            module.rtol = args['rtol']\n",
    "            if args['step_size'] is not None:\n",
    "                module.solver_options['step_size'] = args['step_size']\n",
    "\n",
    "            # Set the test settings\n",
    "            module.test_solver = args.get('test_solver', args['solver'])\n",
    "            module.test_atol = args.get('test_atol', args['atol'])\n",
    "            module.test_rtol = args.get('test_rtol', args['rtol'])\n",
    "\n",
    "        if isinstance(module, ODEfunc):\n",
    "            module.rademacher = args.get('rademacher', True)\n",
    "            module.residual = args.get('residual', False)\n",
    "\n",
    "    model.apply(_set)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.623523Z",
     "start_time": "2023-12-10T16:35:46.618234Z"
    }
   },
   "id": "dfd1407e520ea230"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define loss"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f97171d08d5fa239"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "def standard_normal_logprob(z):\n",
    "    log_z = -0.5 * np.log(2 * np.pi)\n",
    "    return log_z - z.pow(2) / 2"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.623568Z",
     "start_time": "2023-12-10T16:35:46.620238Z"
    }
   },
   "id": "2edd5e64468bf68f"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def bits_per_dim(x, model):\n",
    "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
    "    z, delta_logp = model(x, zero)\n",
    "    \n",
    "    logpz = standard_normal_logprob(z).view(x.shape[0], -1).sum(1, keepdim=True)\n",
    "    logpx = logpz - delta_logp\n",
    "    \n",
    "    logpx_per_dim = torch.sum(logpx) / (x.shape[0] * x.nelement())\n",
    "    \n",
    "    return -(logpx_per_dim - np.log(256)) / np.log(2)\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.625619Z",
     "start_time": "2023-12-10T16:35:46.623048Z"
    }
   },
   "id": "7c882fdf4de57b42"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9e6a755506834321"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def update_lr(optimizer, itr):\n",
    "    iter_frac = min(float(itr + 1) / max(args['warmup_iters'], 1), 1.)\n",
    "    lr = args['lr'] * iter_frac\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-10T16:35:46.635160Z",
     "start_time": "2023-12-10T16:35:46.625954Z"
    }
   },
   "id": "48b34d2069d97541"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu.\n",
      "Number of trainable parameters: 31107.\n",
      "Using batch size 200. Total 300 iterations/epoch.\n",
      "Epoch 1 Iter 0 Loss 8.1062(8.1062) Time 23.8059(23.8059) Total time 5.00(5.00) Grad Norm 0.40\n",
      "Epoch 1 Iter 10 Loss 8.1046(8.1060) Time 25.5334(24.0784) Total time 5.00(5.00) Grad Norm 0.40\n",
      "Epoch 1 Iter 20 Loss 8.1004(8.1052) Time 24.8206(24.2436) Total time 5.00(5.00) Grad Norm 0.38\n",
      "Epoch 1 Iter 30 Loss 8.0964(8.1034) Time 25.4941(24.4282) Total time 5.00(5.00) Grad Norm 0.38\n",
      "Epoch 1 Iter 40 Loss 8.0885(8.1002) Time 24.5258(24.5307) Total time 5.00(5.00) Grad Norm 0.36\n",
      "Epoch 1 Iter 50 Loss 8.0794(8.0959) Time 24.5279(24.5523) Total time 5.00(5.00) Grad Norm 0.34\n",
      "Epoch 1 Iter 60 Loss 8.0698(8.0902) Time 24.6228(24.6278) Total time 5.00(5.00) Grad Norm 0.31\n",
      "Epoch 1 Iter 70 Loss 8.0587(8.0831) Time 24.9202(24.6708) Total time 5.00(5.00) Grad Norm 0.29\n",
      "Epoch 1 Iter 80 Loss 8.0474(8.0750) Time 24.5255(24.6434) Total time 5.00(5.00) Grad Norm 0.26\n",
      "Epoch 1 Iter 90 Loss 8.0345(8.0658) Time 24.4155(24.6281) Total time 5.00(5.00) Grad Norm 0.23\n",
      "Epoch 1 Iter 100 Loss 8.0235(8.0560) Time 24.7740(24.8534) Total time 5.00(5.00) Grad Norm 0.18\n",
      "Epoch 1 Iter 110 Loss 8.0154(8.0464) Time 24.6183(24.8285) Total time 5.00(5.00) Grad Norm 0.12\n",
      "Epoch 1 Iter 120 Loss 8.0110(8.0378) Time 24.1807(24.8374) Total time 5.00(5.00) Grad Norm 0.08\n",
      "Epoch 1 Iter 130 Loss 8.0065(8.0301) Time 25.2217(24.9707) Total time 5.00(5.00) Grad Norm 0.06\n",
      "Epoch 1 Iter 140 Loss 8.0032(8.0236) Time 25.2027(24.9337) Total time 5.00(5.00) Grad Norm 0.06\n",
      "Epoch 1 Iter 150 Loss 8.0007(8.0180) Time 24.5702(24.8500) Total time 5.00(5.00) Grad Norm 0.05\n",
      "Epoch 1 Iter 160 Loss 7.9972(8.0129) Time 25.3591(24.8701) Total time 5.00(5.00) Grad Norm 0.05\n",
      "Epoch 1 Iter 170 Loss 7.9940(8.0084) Time 24.8149(24.9505) Total time 5.00(5.00) Grad Norm 0.05\n",
      "Epoch 1 Iter 180 Loss 7.9906(8.0041) Time 31.3927(26.3470) Total time 5.00(5.00) Grad Norm 0.04\n",
      "Epoch 1 Iter 190 Loss 7.9881(8.0001) Time 28.1010(27.1224) Total time 5.00(5.00) Grad Norm 0.04\n",
      "Epoch 1 Iter 200 Loss 7.9846(7.9964) Time 29.3021(27.4714) Total time 5.00(5.00) Grad Norm 0.04\n",
      "Epoch 1 Iter 210 Loss 7.9829(7.9931) Time 31.2441(27.9790) Total time 5.00(5.00) Grad Norm 0.04\n",
      "Epoch 1 Iter 220 Loss 7.9806(7.9900) Time 31.9973(28.9601) Total time 5.00(5.00) Grad Norm 0.03\n",
      "Epoch 1 Iter 230 Loss 7.9788(7.9873) Time 32.1475(29.8244) Total time 5.00(5.00) Grad Norm 0.03\n",
      "Epoch 1 Iter 240 Loss 7.9776(7.9849) Time 34.5635(30.6045) Total time 5.00(5.00) Grad Norm 0.03\n",
      "Epoch 1 Iter 250 Loss 7.9764(7.9828) Time 38.4373(32.3584) Total time 5.00(5.00) Grad Norm 0.02\n",
      "Epoch 1 Iter 260 Loss 7.9755(7.9810) Time 41.6175(34.2800) Total time 5.00(5.00) Grad Norm 0.02\n",
      "Epoch 1 Iter 270 Loss 7.9745(7.9794) Time 41.5450(36.3072) Total time 5.00(5.00) Grad Norm 0.02\n",
      "Epoch 1 Iter 280 Loss 7.9741(7.9780) Time 42.7057(37.8435) Total time 5.00(5.00) Grad Norm 0.02\n",
      "Epoch 1 Iter 290 Loss 7.9734(7.9769) Time 46.8087(40.0381) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Using batch size 200. Total 300 iterations/epoch.\n",
      "Epoch 2 Iter 300 Loss 7.9732(7.9760) Time 48.6097(42.7413) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 310 Loss 7.9727(7.9752) Time 48.0122(44.1052) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 320 Loss 7.9728(7.9745) Time 94.2822(73.9766) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 330 Loss 7.9724(7.9740) Time 65.6314(75.0562) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 340 Loss 7.9723(7.9735) Time 68.4928(73.2681) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 350 Loss 7.9721(7.9732) Time 49.1018(67.3866) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 360 Loss 7.9719(7.9729) Time 49.4159(62.5545) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 370 Loss 7.9718(7.9726) Time 48.4798(58.8408) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 380 Loss 7.9717(7.9724) Time 43.0962(55.7688) Total time 5.00(5.00) Grad Norm 0.01\n",
      "Epoch 2 Iter 390 Loss 7.9714(7.9722) Time 42.8125(52.3667) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 400 Loss 7.9715(7.9720) Time 41.5288(49.6191) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 410 Loss 7.9715(7.9719) Time 41.9139(47.8257) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 420 Loss 7.9714(7.9717) Time 42.7507(46.5304) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 430 Loss 7.9712(7.9716) Time 41.5368(45.2858) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 440 Loss 7.9712(7.9715) Time 42.1559(44.5725) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 450 Loss 7.9712(7.9714) Time 38.2271(43.0524) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 460 Loss 7.9711(7.9713) Time 38.7537(41.9744) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 470 Loss 7.9710(7.9713) Time 39.7734(41.2874) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 480 Loss 7.9710(7.9712) Time 39.6634(40.8512) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 490 Loss 7.9711(7.9711) Time 39.5978(40.5914) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 500 Loss 7.9709(7.9710) Time 40.0252(40.3900) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 510 Loss 7.9709(7.9710) Time 39.9734(40.3197) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 520 Loss 7.9707(7.9709) Time 40.1677(40.1553) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 530 Loss 7.9707(7.9709) Time 36.8758(60.4408) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 540 Loss 7.9706(7.9708) Time 248.9592(84.6423) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 550 Loss 7.9706(7.9708) Time 36.7746(85.8885) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 560 Loss 7.9705(7.9707) Time 36.9398(73.0555) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 570 Loss 7.9705(7.9707) Time 1079.4103(129.8204) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 580 Loss 7.9705(7.9706) Time 37.0893(120.6403) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 2 Iter 590 Loss 7.9706(7.9706) Time 37.3935(98.7259) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Using batch size 200. Total 300 iterations/epoch.\n",
      "Epoch 3 Iter 600 Loss 7.9705(7.9705) Time 1058.9002(140.1289) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 610 Loss 7.9702(7.9705) Time 36.2846(133.8206) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 620 Loss 7.9704(7.9705) Time 36.3332(108.2373) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 630 Loss 7.9703(7.9704) Time 36.1507(113.7997) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 640 Loss 7.9701(7.9704) Time 35.4719(109.8108) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 650 Loss 7.9703(7.9703) Time 35.6466(90.3539) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 660 Loss 7.9700(7.9703) Time 1009.1707(134.1671) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 670 Loss 7.9698(7.9702) Time 35.5186(122.5228) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 680 Loss 7.9700(7.9702) Time 35.7285(99.7098) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 690 Loss 7.9700(7.9702) Time 35.8401(82.9371) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 700 Loss 7.9702(7.9701) Time 36.0560(70.6235) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 710 Loss 7.9699(7.9701) Time 36.9553(61.6649) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 720 Loss 7.9697(7.9700) Time 42.5651(55.4343) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 730 Loss 7.9699(7.9700) Time 47.2288(51.8799) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 740 Loss 7.9699(7.9700) Time 39.5389(56.4020) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 750 Loss 7.9699(7.9699) Time 51.1544(52.9056) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 760 Loss 7.9697(7.9699) Time 44.8323(50.7663) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 770 Loss 7.9699(7.9699) Time 42.8477(49.2114) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 780 Loss 7.9699(7.9698) Time 44.7954(48.3323) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 790 Loss 7.9698(7.9698) Time 48.1882(47.8022) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 800 Loss 7.9697(7.9698) Time 39.0690(72.1486) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 810 Loss 7.9697(7.9697) Time 41.4501(63.8775) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 820 Loss 7.9696(7.9697) Time 40.7728(84.4632) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 830 Loss 7.9697(7.9697) Time 41.2688(122.7864) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 840 Loss 7.9697(7.9696) Time 41.3768(101.3982) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 850 Loss 7.9693(7.9696) Time 41.1335(114.4341) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 860 Loss 7.9695(7.9696) Time 41.4471(95.2229) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 870 Loss 7.9695(7.9695) Time 41.5636(81.1465) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 880 Loss 7.9695(7.9695) Time 43.5565(71.0028) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 3 Iter 890 Loss 7.9695(7.9695) Time 50.1949(64.4941) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Using batch size 200. Total 300 iterations/epoch.\n",
      "Epoch 4 Iter 900 Loss 7.9694(7.9694) Time 43.7486(83.9701) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 910 Loss 7.9693(7.9694) Time 44.4045(73.4210) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 920 Loss 7.9693(7.9694) Time 54.2714(143.9923) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 930 Loss 7.9692(7.9694) Time 43.1399(117.4902) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 940 Loss 7.9692(7.9693) Time 42.8385(98.1106) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 950 Loss 7.9691(7.9693) Time 43.1562(83.6481) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 960 Loss 7.9693(7.9693) Time 44.6756(73.1650) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 970 Loss 7.9692(7.9693) Time 43.1628(65.2423) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 980 Loss 7.9691(7.9693) Time 45.4483(59.3765) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 990 Loss 7.9691(7.9692) Time 42.3666(55.0620) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 1000 Loss 7.9692(7.9692) Time 45.5623(52.2821) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 1010 Loss 7.9693(7.9692) Time 46.1082(50.6366) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 1020 Loss 7.9692(7.9692) Time 45.6994(49.2331) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 1030 Loss 7.9691(7.9692) Time 46.5299(133.1864) Total time 5.00(5.00) Grad Norm 0.00\n",
      "Epoch 4 Iter 1040 Loss 7.9691(7.9692) Time 57.8675(173.8569) Total time 5.00(5.00) Grad Norm 0.00\n"
     ]
    }
   ],
   "source": [
    "# Get device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device {}.\".format(device))\n",
    "\n",
    "cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
    "\n",
    "set_cnf_options(args, model)\n",
    "\n",
    "print(\"Number of trainable parameters: {}.\".format(sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "\n",
    "if args['resume'] is not None:\n",
    "    checkpoint = torch.load(args['resume'], map_location=lambda storage, loc: storage)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    if 'optim_state_dict' in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint['optim_state_dict'])\n",
    "        # Manually move optimizer state to device\n",
    "        for state in optimizer.state.values():\n",
    "            for k, v in state.items():\n",
    "                if isinstance(v, torch.Tensor):\n",
    "                    state[k] = v.to(device)\n",
    "                    \n",
    "# If possible, parallelize the computation\n",
    "if torch.cuda.is_available():\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    print(\"Parallelizing computation.\")\n",
    "    \n",
    "# Get 100 random samples from normal distribution to visualize the model\n",
    "fixed_z = cvt(torch.randn(100, *data_shape))\n",
    "\n",
    "# Set up the average meters\n",
    "time_meter = RunningAverageMeter(0.97)\n",
    "loss_meter = RunningAverageMeter(0.97)\n",
    "#steps_meter = RunningAverageMeter(0.97)\n",
    "grad_meter = RunningAverageMeter(0.97)\n",
    "tt_meter = RunningAverageMeter(0.97)\n",
    "\n",
    "# Keep track of current best performance\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Start training\n",
    "itr = 0\n",
    "for epoch in range(args['begin_epoch'], args['num_epochs'] + 1):\n",
    "    # Set model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Get train loader\n",
    "    train_loader = get_train_loader(train_set)\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        # Set starting time of epoch iteration\n",
    "        start = time.time()\n",
    "        \n",
    "        # Update learning rate\n",
    "        update_lr(optimizer, itr)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        x = cvt(x)\n",
    "        #Compute loss\n",
    "        loss = bits_per_dim(x, model)\n",
    "        \n",
    "        total_time = count_total_time(model)\n",
    "        \n",
    "        # Backpropagate and update parameters\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 100) # 100 is the max norm\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update average meters\n",
    "        time_meter.update(time.time() - start)\n",
    "        loss_meter.update(loss.item())\n",
    "        grad_meter.update(grad_norm)\n",
    "        #steps_meter.update(count_nfe(model))\n",
    "        tt_meter.update(total_time)\n",
    "        \n",
    "        # Log progress\n",
    "        if itr % args.get('log_freq', 10) == 0:\n",
    "            print(\"Epoch {} Iter {} Loss {:.4f}({:.4f}) Time {:.4f}({:.4f}) Total time {:.2f}({:.2f}) Grad Norm {:.2f}\".format(\n",
    "                epoch, itr, loss_meter.val, loss_meter.avg, time_meter.val, time_meter.avg, tt_meter.val, tt_meter.avg, grad_meter.val\n",
    "            ))\n",
    "            \n",
    "        itr += 1\n",
    "        \n",
    "    # Compute validation loss\n",
    "    model.eval()\n",
    "    \n",
    "    if epoch % args['val_freq'] == 0:\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            print(\"Validating...\")\n",
    "            losses = []\n",
    "            for x, y in test_loader:\n",
    "                x = cvt(x)\n",
    "                losses.append(bits_per_dim(x, model))\n",
    "                \n",
    "            loss = torch.mean(torch.stack(losses))\n",
    "            print(\"Epoch {} Validation Loss {:.4f} Time {:.4f}\".format(epoch, loss, time.time() - start))\n",
    "            \n",
    "            # Save model if it is the best so far\n",
    "            if loss < best_loss:\n",
    "                best_loss = loss\n",
    "                if not os.path.exists('results'):\n",
    "                    os.makedirs('results')\n",
    "                torch.save({\n",
    "                    'args': args,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optim_state_dict': optimizer.state_dict()\n",
    "                }, 'models/mnist_odenvp_best.pth')\n",
    "\n",
    "    # Visualize the model\n",
    "    with torch.no_grad():\n",
    "        fig_filename = os.path.join('mnist_results', \"{}.jpg\".format(epoch))\n",
    "        if not os.path.exists('mnist_results'):\n",
    "            os.makedirs('mnist_results')\n",
    "        generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
    "        save_image(generated_samples, fig_filename, nrow=10)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-10T16:35:46.632442Z"
    }
   },
   "id": "e9f72e223d4d73b1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "7ecdd9865f9aa2d4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
